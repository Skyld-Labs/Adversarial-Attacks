# Adversarial Attacks on Object Detection Models

This repository demonstrates white-box adversarial attacks on object detection models, specifically targeting YOLOv5. It provides implementations of Projected Gradient Descent (PGD) and Adversarial Patch attacks with interactive Jupyter notebooks for experimentation and analysis.

## Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Available Attacks](#available-attacks)
- [Usage Examples](#usage-examples)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## Overview

This project implements and demonstrates adversarial attacks against object detection models. The attacks are designed to fool YOLOv5 models by either:
- **PGD Attack**: Adding imperceptible noise to entire images
- **Adversarial Patch Attack**: Placing visible patches that cause misclassification

The repository includes interactive notebooks that allow you to:
- Configure attack parameters
- Generate adversarial examples
- Visualize attack results
- Evaluate attack effectiveness

## Features

- üéØ **Multiple Attack Types**: PGD and Adversarial Patch implementations
- üìä **Interactive Notebooks**: Easy-to-use Jupyter notebooks for experimentation
- ‚öôÔ∏è **Configurable Parameters**: JSON-based configuration system
- üìà **Visualization**: Built-in plotting and analysis tools
- üé® **Custom Datasets**: Support for custom image datasets

## Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd Adversarial-Attacks
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Download YOLOv5 model** (if not already present):
   The YOLOv5s model (`yolov5s.pt`) should be placed in the `data/` directory.

## Project Structure

```
Adversarial-Attacks/
‚îú‚îÄ‚îÄ README.md                                    # This file
‚îú‚îÄ‚îÄ requirements.txt                             # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                                   # Git ignore rules
‚îú‚îÄ‚îÄ data/                                        # Data and configuration files
‚îÇ   ‚îú‚îÄ‚îÄ yolov5s.pt                              # YOLOv5 model weights
‚îÇ   ‚îú‚îÄ‚îÄ configs/                                 # Configuration files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pgd_default_config.json             # Default PGD configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adv_patch_default_config.json       # Default Patch configuration
‚îÇ   ‚îú‚îÄ‚îÄ custom_images/                           # Input images for attacks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ airplane0.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ car0.JPG
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ original/                                # Original detection results
‚îÇ   ‚îú‚îÄ‚îÄ ProjectedGradientDescent/                # PGD attack results
‚îÇ   ‚îî‚îÄ‚îÄ AdversarialPatchPytorch/                 # Patch attack results
‚îÇ       ‚îú‚îÄ‚îÄ AdversarialPatchPytorch0.png
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ src/                                         # Source code
    ‚îú‚îÄ‚îÄ notebook_white_box_pgd.ipynb            # PGD attack notebook
    ‚îú‚îÄ‚îÄ notebook_white_box_adversarial_patch.ipynb  # Patch attack notebook
    ‚îú‚îÄ‚îÄ attacks/                                 # Attack implementations
    ‚îÇ   ‚îî‚îÄ‚îÄ white_boxes/                         # White-box attacks
    ‚îÇ       ‚îú‚îÄ‚îÄ local_projected_gradient_descent.py
    ‚îÇ       ‚îú‚îÄ‚îÄ local_adversarial_patch_pytorch.py
    ‚îÇ       ‚îî‚îÄ‚îÄ white_box_attack.py
    ‚îî‚îÄ‚îÄ utils/                                   # Utility functions
        ‚îú‚îÄ‚îÄ utils.py                             # General utilities
        ‚îî‚îÄ‚îÄ utils_detector_yolo.py               # YOLO-specific utilities
```

## Quick Start

1. **Navigate to the source directory**:
   ```bash
   cd src/
   ```

2. **Start Jupyter Notebook**:
   ```bash
   jupyter notebook
   ```

3. **Open a notebook**:
   - For PGD attacks: `notebook_white_box_pgd.ipynb`
   - For patch attacks: `notebook_white_box_adversarial_patch.ipynb`

4. **Run the cells** following the step-by-step instructions in the notebook.

## Configuration

Attacks are configured using JSON files in the `data/configs/` directory. The configuration includes:

### PGD Attack Parameters
```json
{
    "THRESHOLD" : 0.6,
    "TARGET_CLASS" : "bird",
    "VICTIM_CLASS" : "airplane",
    "BATCH_SIZE" : 5,
    "IOU_THRESHOLD" : 0.5,
    "MAX_ITER" : 500,
    "NORM" : "inf",
    "EPS" : 1.0,
    "TARGET_LOCATION" : [170, 170],
    "TARGET_SHAPE" : [3, 300, 300],
    "IMAGES_TO_DISPLAY" : 3,
    "FOLDER_NAME" : "original"
}
```

### Adversarial Patch Parameters
```json
{
    "THRESHOLD" : 0.6,
    "TARGET_CLASS" : "",
    "VICTIM_CLASS" : "car",
    "BATCH_SIZE" : 5,
    "IOU_THRESHOLD" : 0.5,
    "MAX_ITER" : 500,
    "NORM" : "inf",
    "EPS" : 1.0,
    "TARGET_LOCATION" : [200, 200],
    "TARGET_SHAPE" : [3, 100, 100],
    "IMAGES_TO_DISPLAY" : 3,
    "FOLDER_NAME" : "original"
}
```

### Configuration Parameters
- **`THRESHOLD`**: Confidence threshold for filtering predictions
- **`TARGET_CLASS`**: Target class for attack (null for untargeted)
- **`VICTIM_CLASS`**: Class to attack (empty string for all classes)
- **`BATCH_SIZE`**: Number of images to process
- **`MAX_ITER`**: Number of attack iterations
- **`NORM`**: Perturbation norm ("1", "2", or "inf")
- **`EPS`**: Maximum perturbation magnitude

## Available Attacks

### 1. Projected Gradient Descent (PGD)
- **Type**: White-box attack
- **Method**: Iterative gradient-based optimization
- **Target**: Entire image with imperceptible noise
- **Notebook**: `notebook_white_box_pgd.ipynb`

### 2. Adversarial Patch
- **Type**: White-box attack
- **Method**: Optimized visible patches
- **Target**: Localized regions in images
- **Notebook**: `notebook_white_box_adversarial_patch.ipynb`

## Usage Examples

### Running a PGD Attack
```python
# Load configuration
config = load_config("../data/configs/pgd_default_config.json")

# Create detector
detector = UtilsDetectorYolo(config.BATCH_SIZE, config.THRESHOLD, config.VICTIM_CLASS)

# Initialize attack
attack = LocalProjectedGradientDescent(
    estimator=detector,
    images=detector.images,
    orig_predictions=original_predictions,
    target_class=config.TARGET_CLASS
)

# Generate adversarial examples
attack.generate(
    images=detector.images,
    norm=config.NORM,
    eps=config.EPS,
    max_iter=config.MAX_ITER
)

# Apply the attack to the images 
attack_predictions, adversarial_examples = attack.apply_attack_to_image(
    image=IMAGES,
    train_on=len(IMAGES),
    threshold=config.THRESHOLD,
)
```

### Running an Adversarial Patch Attack
```python
# Load configuration
config = load_config("../data/configs/adv_patch_default_config.json")

# Create estimator
estimated_object_detector = UtilsDetectorYolo(config.BATCH_SIZE, config.THRESHOLD, config.VICTIM_CLASS)

# Initialize attack
attack = LocalAdversarialPatchPytorch(
    estimator=estimated_object_detector,
    images=IMAGES,
    orig_predictions=original_predictions, 
    target_class=config.TARGET_CLASS
    )

# Generate the patch attack
patch, _ = attack.generate(
    images = IMAGES,
    orig_predictions = original_predictions,
    target_shape = config.TARGET_SHAPE,
    target_location = config.TARGET_LOCATION,
    max_iter = config.MAX_ITER
)

# Apply the attack to the images 
attack_predictions, adversarial_examples = attack.apply_attack_to_image(
    image=IMAGES,
    train_on=len(IMAGES),
    threshold=config.THRESHOLD,
)
```

### Custom Image Dataset
Place your images in `data/custom_images/` and update the configuration:
```json
{
  "FOLDER_NAME": "custom_images",
  "VICTIM_CLASS": "person"
}
```

## Results

Attack results are automatically saved in organized folders:
- **Original detections**: `data/original/`
- **PGD results**: `data/ProjectedGradientDescent/`
- **Patch results**: `data/AdversarialPatchPytorch/`

Each result includes:
- Visualized detection boxes
- Generated adversarial examples

## Requirements

- Python 3.8+
- PyTorch
- torchvision
- NumPy
- Matplotlib
- OpenCV
- ART (Adversarial Robustness Toolbox)
- YOLOv5
- Jupyter Notebook

See `requirements.txt` for complete dependency list.

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-attack`)
3. Commit your changes (`git commit -am 'Add new attack method'`)
4. Push to the branch (`git push origin feature/new-attack`)
5. Create a Pull Request


## Acknowledgments

- YOLOv5 by Ultralytics
- Adversarial Robustness Toolbox (ART)

---

**‚ö†Ô∏è Disclaimer**: This tool is for research and educational purposes only. Use responsibly and in accordance with applicable laws and regulations.