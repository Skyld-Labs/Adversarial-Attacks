from attacks.white_boxes.white_box_attack import WhiteBoxAttack

from art.attacks.evasion import ProjectedGradientDescent

import numpy as np

class LocalProjectedGradientDescent(WhiteBoxAttack):
    """
    Implementation of the Local Projected Gradient Descent (PGD) attack.

    This attack generates adversarial examples by applying perturbations to images using the PGD method.
    It supports both targeted and untargeted attacks.
    """

    def __init__(self, estimator, images, orig_predictions, target_class=None):
        """
        Initialize the LocalProjectedGradientDescent class.

        Args:
            estimator (object): The object detection model estimator.
            images (np.ndarray): Array of images to apply the attack on. Shape: (BATCH, C, H, W).
            orig_predictions (list): Original predictions in the format:
                [
                    ([class_name1, ...], [[(y1, x1), (y2, x2)], ...], [score1, ...]),
                    ...
                ].
            target_class (str, optional): The target class for the attack. Defaults to None.
        """
        super().__init__(estimator, images, orig_predictions, target_class)
        self.attack_name = "ProjectedGradientDescent"

        self.target = None
        
        
    
    def generate(self, images, target_shape, target_location, targets=None, orig_predictions=(), norm=np.inf, eps=0.3, max_iter=1000):
        """
        Configure the parameters for the PGD attack and instantiate the corresponding attack object.

        Args:
            images (np.ndarray): The input images to attack. Shape: (BATCH, C, H, W).
            target_shape (tuple): Useless parameter for this attack, but kept for compatibility.
            target_location (tuple): Useless parameter for this attack, but kept for compatibility.
            targets (list, optional): List of target dictionaries for the attack. Defaults to None.
            orig_predictions (tuple, optional): Useless parameter for this attack, but kept for compatibility. Defaults to ().
            norm (float): The norm to use for the attack (default is np.inf). It can be np.inf, 1, or 2.
            eps (float): The maximum perturbation allowed for the attack.
            max_iter (int): Maximum number of iterations for the attack.

        Returns:
            tuple: A tuple containing:
                - np.ndarray: An array of zeros (placeholder for compatibility).
                - ProjectedGradientDescent: The attack object.
        """
        # Parameters for the attack
        detector = self.estimator.estimator
        max_iter = max_iter

        # Create the ProjectedGradientDescent attack object with specified parameters
        self.attack = ProjectedGradientDescent(
            estimator=detector,
            max_iter=max_iter,
            targeted=(self.target_class != ''),
            norm=norm,
            eps=eps,
        )

        self.norm = norm
        self.eps = eps

        if self.target_class != '':
            if targets is None:
                # Create a fake target for the attack
                targets = [self.generate_fake_target(target_location, target_shape) for _ in range(len(images))]
            self.target = targets

        return np.zeros((1), float), self.attack

    def apply_attack_to_image(self, image=[], train_on=1, image_name="", threshold=0.5, plot_and_predictions=True):
        """
        Generate adversarial examples by applying the attack to the provided images.

        Args:
            image (np.ndarray): The input image(s) to which the attack will be applied.
                            Images must be the same as the ones used for generating the attack.
                            The shape should be (BATCH_SIZE, C, H, W) or (C, H, W).
            train_on (int, optional): Number of images used to train the patch. Defaults to 1.
            image_name (str, optional): The name of the image for saving purposes. Defaults to "".
            threshold (float, optional): Threshold for filtering predictions. Defaults to 0.5.
            plot_and_predictions (bool, optional): Whether to plot the image and generate predictions. Defaults to True.

        Returns:
            tuple: A tuple containing:
            - predictions (list): List of predictions after applying the attack.
            - adversarial_example (np.ndarray): The adversarial example(s) generated by the attack.

        Raises:
            ValueError: If the attack or target has not been generated.
        """
        if not self.attack:
            raise ValueError("The attack has not been generated. Call generate() first.")
        if self.target_class != '':
            if self.target is None:
                raise ValueError("Target must be generated for the attack.")
        
        if len(image) == 0:
            image = self.images

        if len(image.shape) == 3:
            images = [image]
        elif len(image.shape) == 4:
            images = image
        else:
            raise ValueError("Image should be of shape (C, H, W) or (BATCH_SIZE, C, H, W).")

        if len(image_name) == 0:
            image_name = f"ProjectedGradientDescent_L{self.norm}_eps_{self.eps}_to_{self.target_class}{'_' if len(image.shape) == 4 else ''}"
        dif_name = f"dif_L{self.norm}_eps_{self.eps}_to_{self.target_class}_"


        if self.target_class:
            y = self.target
        else:
            if self.orig_predictions is None:
                raise ValueError("The prediction made by the original model must be provided for the attack.")
            y = self.estimator.prediction_format_to_art_format(self.orig_predictions)

        edited_images = self.attack.generate(x=images, y=y)
        for i in range(len(images)):
            e = edited_images[i]
            dif = images[i]-e
            self.estimator.plot_image(
                img=dif.transpose(1, 2, 0).copy(),
                boxes=[],
                pred_cls=[],
                title=f"{dif_name}{i}",
                folder=f"../data/{self.attack_name}/"
            )

        self.adversarial_example = np.array(edited_images)

        if plot_and_predictions:
            return super().apply_attack_to_image(
                image=image,
                train_on=train_on,
                image_name=image_name,
                threshold=threshold,
                plot_and_predictions=plot_and_predictions
            )
        return [], self.adversarial_example